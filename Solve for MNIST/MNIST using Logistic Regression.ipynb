{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #imports numpy as np. Numpy is used for basic matrix multiplication, addition, subtraction etc.\n",
    "import tensorflow as tf #imports tensorflow as tf. Tensorflow is an n-dimensional matrix, just like a 1-D vector, 2-D array, 3-D array etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data #imports mnist input data from tensorflow examples. \n",
    "#Mnist data set consists of images of numbers from 0-9, each image is a 28*28 dimensional. There are total 60k training images and 10k test images.\n",
    "mnist = input_data.read_data_sets(\"MNIST/data\", one_hot = True) #using input data call read data sets from a folder MNIST/data and store in mnist.\n",
    "#one hot vector is used which means at once only one class will be true. Since our images have labels 0-9 that means out of all 10 classes only 1 class will be true at a time rest all will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "learning_rate = 0.01 #learning rate is used to reduce our cost/loss/cross entropy and helps in converging or reaching the local optima. The learning rate should neither be too high or too low it should be a balanced rate. \n",
    "training_epochs = 25 #number of times we train our network, its like a loop which trains our network, calculates cost, optimizes it in every epoch, also in every epoch we take \n",
    "display_step = 1 #after how many epochs we want to output our desired results on screen\n",
    "batch_size = 100 #this means that our training images will be divided in a fixed batch size and at every batch it will take a fixed number of images and train them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholder is like a variable to which we will assign data later on. It will allow us to do operations and build our computation graph without feeding in data.\n",
    "#x will hold the training images in form of matrix,the dimensions of x will be in our case None*784, that is why we use None which allows us to vary the dimensionality of our rows.\n",
    "#we use float to define its type.\n",
    "x = tf.placeholder(\"float\",[None,784]) \n",
    "#similarly y will hold the label of the training images in form matrix which will be a None*10 matrix. None will be replaced by the number of images we want to train on.\n",
    "y = tf.placeholder(\"float\",[None,10])\n",
    "\n",
    "#hyperparameters\n",
    "W = tf.Variable(tf.zeros([784,10])) #we use variable instead of placeholder because a variable maintains state in the graph across calls to run(), we initially define Weights as all zeros which are then randomly changed in every epoch. The dimensions of W have to be in alignment with the dimensions of x as we have to multiply them in later stages.\n",
    "b = tf.Variable(tf.zeros([10])) #we use a 10-D column vector which we then add with the result we get from x * W.\n",
    "\n",
    "#here we do the prediction by using softmax classifier, we can also use ReLu, Sigmoid, Tan etc. We get a value between 0 and 1 which we then try to predict is for which particular class.\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#here we calculate the loss, how far our predicted value is from the actual output class label, which we then try to minimize.\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices = 1))\n",
    "\n",
    "#here we do the optimization where we try to minimize our cost function, by running GradientDescentOptimizer(backpropogate through the network) and use a learning rate which helps in achieving local optima.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we initialise all the variables in our model.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.184184278\n",
      "Epoch: 0002 cost= 0.665409036\n",
      "Epoch: 0003 cost= 0.552806490\n",
      "Epoch: 0004 cost= 0.498703455\n",
      "Epoch: 0005 cost= 0.465472227\n",
      "Epoch: 0006 cost= 0.442540389\n",
      "Epoch: 0007 cost= 0.425520185\n",
      "Epoch: 0008 cost= 0.412190600\n",
      "Epoch: 0009 cost= 0.401376767\n",
      "Epoch: 0010 cost= 0.392374381\n",
      "Epoch: 0011 cost= 0.384781689\n",
      "Epoch: 0012 cost= 0.378152853\n",
      "Epoch: 0013 cost= 0.372400416\n",
      "Epoch: 0014 cost= 0.367281650\n",
      "Epoch: 0015 cost= 0.362726723\n",
      "Epoch: 0016 cost= 0.358613340\n",
      "Epoch: 0017 cost= 0.354812526\n",
      "Epoch: 0018 cost= 0.351458141\n",
      "Epoch: 0019 cost= 0.348335585\n",
      "Epoch: 0020 cost= 0.345452889\n",
      "Epoch: 0021 cost= 0.342713490\n",
      "Epoch: 0022 cost= 0.340214653\n",
      "Epoch: 0023 cost= 0.337925483\n",
      "Epoch: 0024 cost= 0.335768323\n",
      "Epoch: 0025 cost= 0.333716870\n",
      "Optimization Finished!\n",
      "Accuracy: 0.888667\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph. #this is a class that runs all the tensorflow operations and launches the graph in a session. All the operations have to be within the indentation. \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) ##sess.run(init), runs the variables that were initialised in the previous step and evaluates the tensor \n",
    "\n",
    "    # Training cycle\n",
    "    #we use for loop, to loop around all the 25 training_epochs.\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0. #initialize avg_cost to zero.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) #here we input our batch size and we store the output in total_batch. For eg: we have used 100 as batch size so our total_batch will be 550. You can try printing the value to see how it varies based on the batch size.\n",
    "        \n",
    "        for i in range(total_batch):   #now we iterate over each total_batch starting from 0-549\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #at each i we load 100 images and there labels in batch_xs and batch_ys, which are nothing but a matrix representation of images.\n",
    "            \n",
    "            # Fit training using batch data, think it as optimizer and cost being a root node and x and y their child or parent node.\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,       \n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss                                            \n",
    "            avg_cost += c / total_batch  #we calculate cost for 100 images once at a time and every time we get next 100 images and this keeps on going for 550 times.\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d'%(epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    print \"Optimization Finished!\"    \n",
    "\n",
    "    # Test model, here we check whether the index of the maximum value of the predicted image is equal to the actual labelled image and both will be a column vector.\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    #calculate accuracy across the correct_prediction using reduce_mean. For eg: if we have 10 classes and out of which only 4 classes predicted result was true so we will get something like 4/10 as accuracy.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
