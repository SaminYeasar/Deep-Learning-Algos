{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis : analysis of IMDB review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set description\n",
    "IMDB dataset :\n",
    "* IMDB dataset consists of 15000 reviews in training set, 10000 reviews in validation set, and 25000 reviews in test set. This is a 2 class problem with class 1 being positive sentiment and class 0 being negative sentiment.\n",
    "\n",
    "Preprocess of the data:\n",
    "* Excluded all punctuation and converted everything into lower case\n",
    "* Made a vocabulary of 10000 most common words from training data and discarded anything not found within the vocabulary.\n",
    "* constructed bag of words, as in converted text into number that works as features.\n",
    "\n",
    "Classifer used:\n",
    "* As a baseline, report the performance of the random classiffier (a classiffier which classiffies a review into an uniformly random class). And then have done comparative studies using Naive Bayes, Decision Trees, and Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#loc = 'D:/University materials/Winter 2018/Applied ML/winter 2018/Assignments/assignment 3/hwk3_datasets/IMDB-train.txt'\n",
    "loc = 'IMDB-train.txt'\n",
    "IMDB_train = pd.read_table(loc,header=None,names=['review','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = IMDB_train.label.unique()\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frq_vect = [] # frequency vector; counts the number of time lables got repeated\n",
    "for itr in range ( 0,len(labels) ):\n",
    "    frq_vect = np.append (frq_vect, (sum(IMDB_train.label==labels[itr])) ) \n",
    "    \n",
    "No_examples = len(IMDB_train.label)\n",
    "class_prob = frq_vect/No_examples # probability of perticular class occurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Classifier computing F1 score for IMDB train,test,valid data prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomPred (labels,No_examples,class_prob,true_labels):\n",
    "    # prediticng class with random classifier and getting F1 score\n",
    "    pred_labels = np.random.choice(labels, No_examples, p=class_prob)\n",
    "    from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "    IMDB_f1 = f1_score(true_labels,pred_labels,average='micro')\n",
    "    IMDB_acc = accuracy_score(true_labels,pred_labels)\n",
    "\n",
    "    print('F1 score',IMDB_f1)\n",
    "    print('Accuracy',IMDB_acc)\n",
    "    return (IMDB_f1,IMDB_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score for IMDB training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.5068666666666667\n",
      "Accuracy 0.5068666666666667\n"
     ]
    }
   ],
   "source": [
    "No_examples = len(IMDB_train.label)\n",
    "true_labels = np.asarray(IMDB_train.label)\n",
    "f1_train,train_acc = getRandomPred (labels,No_examples,class_prob,true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score for IMDBvalidaion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.5038\n",
      "Accuracy 0.5038\n"
     ]
    }
   ],
   "source": [
    "#loc = 'D:/University materials/Winter 2018/Applied ML/winter 2018/Assignments/assignment 3/hwk3_datasets/IMDB-valid.txt'\n",
    "loc = 'IMDB-valid.txt'\n",
    "IMDB_valid = pd.read_table(loc,header=None,names=['review','label'])\n",
    "# for IMDB training data\n",
    "true_labels = np.asarray(IMDB_valid.label)\n",
    "No_examples = len(IMDB_valid.label)\n",
    "f1_valid,valid_acc = getRandomPred (labels,No_examples,class_prob,true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score for IMDB testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.50136\n",
      "Accuracy 0.50136\n"
     ]
    }
   ],
   "source": [
    "#loc = 'D:/University materials/Winter 2018/Applied ML/winter 2018/Assignments/assignment 3/hwk3_datasets/IMDB-test.txt'\n",
    "loc = 'IMDB-test.txt'\n",
    "IMDB_test = pd.read_table(loc,header=None,names=['review','label'])\n",
    "# for IMDB training data\n",
    "true_labels = np.asarray(IMDB_test.label)\n",
    "No_examples = len(IMDB_test.label)\n",
    "f1_test,test_acc = getRandomPred (labels,No_examples,class_prob,true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Classification IMDB data using Binary bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextPreprocess(text):   \n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace('[^\\w\\s]','')    \n",
    "    text = text.str.replace('[0-9]','')\n",
    "    text = text.str.replace('_','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('IMDB-vocab.txt',header=None,names = ['word','id','frequency'])\n",
    "#vocabtotext = vocab.word.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc = 'D:/University materials/Winter 2018/Applied ML/winter 2018/Assignments/assignment 3/hwk3_datasets/IMDB-train.txt'\n",
    "loc = 'IMDB-train.txt'\n",
    "IMDB_train = pd.read_table(loc,header=None,names=['review','label'])\n",
    "IMDB_train.review = TextPreprocess(IMDB_train.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc = 'D:/University materials/Winter 2018/Applied ML/winter 2018/Assignments/assignment 3/hwk3_datasets/IMDB-test.txt'\n",
    "loc = 'IMDB-test.txt'\n",
    "IMDB_test = pd.read_table(loc,header=None,names=['review','label'])\n",
    "IMDB_test.review = TextPreprocess(IMDB_test.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc = 'D:/University materials/Winter 2018/Applied ML/winter 2018/Assignments/assignment 3/hwk3_datasets/IMDB-valid.txt'\n",
    "loc = 'IMDB-valid.txt'\n",
    "IMDB_valid = pd.read_table(loc,header=None,names=['review','label'])\n",
    "IMDB_valid.review = TextPreprocess(IMDB_valid.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_counts(doc): \n",
    "    #myDictionary = collections.OrderedDict()\n",
    "    import collections\n",
    "    myDictionary = {}  \n",
    "    myFile = doc\n",
    "    field = myFile.split()\n",
    "    frequency=collections.Counter(field) #for bag of frequency\n",
    "    \n",
    "    #for line in range(len(field)):\n",
    "        #myDictionary[line] = [field[line] , line]\n",
    "        #myDictionary[field[line]] =  line\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparse_matrix(texts, vocab):\n",
    "    \n",
    "    from scipy.sparse import csr_matrix\n",
    "    \"\"\" Generate a sparse matrix from the given texts, using doc_counts function \"\"\"\n",
    "    D = len(texts)\n",
    "    V = len(vocab.word)\n",
    "        \n",
    "    mat_bag_data = []\n",
    "    mat_freq_data = []\n",
    "    mat_indptr = [0]\n",
    "    mat_indices = []\n",
    "\n",
    "    for i,doc in enumerate(texts):\n",
    "      #  counts,frequency = doc_counts(doc) # counts basically nested list contains words\n",
    "                                            # frequency hold the number repeatations\n",
    "        frequency = doc_counts(doc) \n",
    "        \n",
    "        #N = len(counts)  # idk why N requires \n",
    "        used = 0\n",
    "        for word,count in frequency.items():\n",
    "            if vocab.loc[vocab.word== word].empty:\n",
    "                # if the word is missing in vocab we skip it\n",
    "                continue                        \n",
    "            else:\n",
    "                index = vocab[vocab.word == word].iloc[0].id - 1  \n",
    "                # -1 cause our id starts from 1 but in matrix first indice is 0, so we get 10001 example matrix as we skipped 0\n",
    "                # so -1 so that\n",
    "                #print(word)\n",
    "                mat_indices.append(index)\n",
    "                mat_bag_data.append(1)\n",
    "                mat_freq_data.append(frequency[word])\n",
    "                used += 1\n",
    "        mat_indptr.append(mat_indptr[-1] + used)\n",
    "        \n",
    "    mat_bag = csr_matrix((mat_bag_data, mat_indices, mat_indptr), (D,V+1), dtype='int')\n",
    "    mat_freq = csr_matrix((mat_freq_data, mat_indices, mat_indptr), (D,V+1), dtype='int') \n",
    "    # mat_freq has not normalized, normalize it while using it in classifier \n",
    "    #mat[:,0] = 1\n",
    "    \n",
    "        \n",
    "    return mat_bag,mat_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "'''\n",
    "One sprase matrix constructed save it and call the data when its needed as computation is time consuming\n",
    "'''\n",
    "\n",
    "############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing sparse matrix of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 21min 37s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "%lsmagic\n",
    "%time mat_bag,mat_freq = generate_sparse_matrix(IMDB_train.review, vocab)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz('IMDB_train_bag_mat.npz', mat_bag)\n",
    "scipy.sparse.save_npz('IMDB_train_freq_mat.npz', mat_freq)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computing sparse matrix of testing data\n",
    "### computing sparse matrix of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7h 44min 18s\n",
      "Wall time: 56min 46s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "%lsmagic\n",
    "%time test_mat_bag,test_mat_freq = generate_sparse_matrix(IMDB_test.review, vocab)\n",
    "%time valid_mat_bag,valid_mat_freq = generate_sparse_matrix(IMDB_valid.review, vocab)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scipy.sparse.save_npz('IMDB_test_bag_mat.npz', test_mat_bag)\n",
    "scipy.sparse.save_npz('IMDB_test_freq_mat.npz', test_mat_freq)\n",
    "\n",
    "scipy.sparse.save_npz('IMDB_valid_bag_mat.npz', valid_mat_bag)\n",
    "scipy.sparse.save_npz('IMDB_valid_freq_mat.npz', valid_mat_freq)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis if Classification Efficiency using bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "train_mat = scipy.sparse.load_npz('IMDB_train_bag_mat.npz')\n",
    "test_mat = scipy.sparse.load_npz('IMDB_test_bag_mat.npz')\n",
    "valid_mat = scipy.sparse.load_npz('IMDB_valid_bag_mat.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set = IMDB_train.iloc[np.random.choice(len(IMDB_train.review),20)]       # randomly picking dataset to train\n",
    "#validation_set = IMDB_valid.iloc[np.random.choice(len(IMDB_valid.review),20)]      # randomly picking dataset to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_true = IMDB_train.label\n",
    "valid_y_true = IMDB_valid.label\n",
    "test_y_true = IMDB_test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computeing accuracy and F1 score for predicting \"training data\"\n",
    "# input sparse matrix data that needs to be predicted\n",
    "def getClassifierEff (Data,true_y,clf):\n",
    "    from sklearn.metrics import f1_score,accuracy_score\n",
    "    y_pred = clf.predict(Data)\n",
    "    acc = accuracy_score(true_y,y_pred)\n",
    "    f1 = f1_score(true_y,y_pred,average='micro')\n",
    "    return f1,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training classifier with \"training data\"\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machine learning\\python 3.5.2\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "Hyp = [0, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1]\n",
    "IMDB_valid_f1 = []\n",
    "IMDB_valid_acc = []\n",
    "for itr in range(len(Hyp)):\n",
    "    naive_clf = BernoulliNB(alpha=Hyp[itr]).fit(train_mat, train_y_true)\n",
    "    f1 , acc = getClassifierEff (valid_mat,valid_y_true,naive_clf)\n",
    "    IMDB_valid_f1.append(f1)\n",
    "    IMDB_valid_acc.append(acc)\n",
    "   # print('IMDB_valid_f1',IMDB_valid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.841,\n",
       " 0.841,\n",
       " 0.8412,\n",
       " 0.8413000000000002,\n",
       " 0.8416,\n",
       " 0.8415,\n",
       " 0.8416999999999999,\n",
       " 0.8421,\n",
       " 0.8422999999999999,\n",
       " 0.8428,\n",
       " 0.8433,\n",
       " 0.8424000000000001]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB_valid_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for hyper parameter alpha = 0.1 we get max F1 score\n"
     ]
    }
   ],
   "source": [
    "optim_alpha = Hyp[np.argmax(IMDB_valid_f1)]\n",
    "print('for hyper parameter alpha =',Hyp[np.argmax(IMDB_valid_f1)],'we get max F1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_train_f1 0.8707333333333334\n",
      "IMDB_test_f1 0.8318399999999999\n"
     ]
    }
   ],
   "source": [
    "naive_clf = BernoulliNB( alpha=optim_alpha ).fit(train_mat, train_y_true)\n",
    "\n",
    "train_f1 , train_acc = getClassifierEff (train_mat,train_y_true,naive_clf)\n",
    "test_f1 , test_acc = getClassifierEff (test_mat,test_y_true,naive_clf)\n",
    "\n",
    "print('IMDB_train_f1',train_f1)\n",
    "print('IMDB_test_f1',test_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = ['gini','entropy'] #criterion \n",
    "split = ['best','random'] #splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_valid_f1 0.6894 when we use criterion gini and splitter best\n",
      "IMDB_valid_f1 0.6979 when we use criterion gini and splitter random\n",
      "IMDB_valid_f1 0.6947 when we use criterion entropy and splitter best\n",
      "IMDB_valid_f1 0.6966 when we use criterion entropy and splitter random\n"
     ]
    }
   ],
   "source": [
    "IMDB_valid_f1 = []\n",
    "IMDB_valid_acc = []\n",
    "for itr1 in range(len(crit)):\n",
    "    for itr2 in range(len(split)):\n",
    "        tree_clf = tree.DecisionTreeClassifier(criterion = crit[itr1], splitter = split[itr2]).fit(train_mat, train_y_true)\n",
    "        f1,acc = getClassifierEff (valid_mat,valid_y_true,tree_clf)\n",
    "        IMDB_valid_f1.append(f1)\n",
    "        IMDB_valid_acc.append(acc)\n",
    "        \n",
    "        print('IMDB_valid_f1',f1,'when we use criterion',crit[itr1],'and splitter',split[itr2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6979"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(IMDB_valid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_train_f1 1.0\n",
      "IMDB_test_f1 0.69776\n"
     ]
    }
   ],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(criterion = 'gini', splitter = 'random').fit(train_mat, train_y_true)\n",
    "IMDB_train_f1,IMDB_train_acc = getClassifierEff (train_mat,train_y_true,tree_clf)\n",
    "IMDB_test_f1,IMDB_test_acc = getClassifierEff (test_mat,test_y_true,tree_clf)\n",
    "\n",
    "print('IMDB_train_f1',IMDB_train_f1)\n",
    "print('IMDB_test_f1',IMDB_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_clf = LinearSVC().fit(train_mat, train_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for combination of penalty='l1','l2' , loss='squred_hinge',dual=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen = ['l1','l2']\n",
    "los = ['squared_hinge'] # gives error for hinge\n",
    "dul = [False] # gives error for dual = true. \n",
    "tolerance = [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1 ]\n",
    "C_param = [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_valid_f1 0.8454 when we use penalty l1\n",
      "IMDB_valid_f1 0.8428 when we use penalty l2\n"
     ]
    }
   ],
   "source": [
    "IMDB_valid_f1 = []\n",
    "IMDB_valid_acc = []\n",
    "for itr1 in range(len(pen)):\n",
    "\n",
    "    linear_clf = LinearSVC(penalty=pen[itr1] , dual=False).fit(train_mat, train_y_true)\n",
    "    f1 , acc = getClassifierEff (valid_mat,valid_y_true,linear_clf)\n",
    "    IMDB_valid_f1.append(f1)\n",
    "    IMDB_valid_acc.append(acc)\n",
    "            \n",
    "    print('IMDB_valid_f1',f1,'when we use penalty',pen[itr1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-10 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 1e-10 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 1e-10 C 0.01\n",
      "IMDB_valid_f1 0.869 when we use penalty l1 tolerence 1e-10 C 0.1\n",
      "IMDB_valid_f1 0.8455 when we use penalty l1 tolerence 1e-10 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-09 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 1e-09 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 1e-09 C 0.01\n",
      "IMDB_valid_f1 0.8687 when we use penalty l1 tolerence 1e-09 C 0.1\n",
      "IMDB_valid_f1 0.8469000000000001 when we use penalty l1 tolerence 1e-09 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-08 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 1e-08 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 1e-08 C 0.01\n",
      "IMDB_valid_f1 0.8691000000000001 when we use penalty l1 tolerence 1e-08 C 0.1\n",
      "IMDB_valid_f1 0.8472 when we use penalty l1 tolerence 1e-08 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-07 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 1e-07 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 1e-07 C 0.01\n",
      "IMDB_valid_f1 0.8691000000000001 when we use penalty l1 tolerence 1e-07 C 0.1\n",
      "IMDB_valid_f1 0.8451 when we use penalty l1 tolerence 1e-07 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-06 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 1e-06 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 1e-06 C 0.01\n",
      "IMDB_valid_f1 0.869 when we use penalty l1 tolerence 1e-06 C 0.1\n",
      "IMDB_valid_f1 0.8449 when we use penalty l1 tolerence 1e-06 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1e-05 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 1e-05 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 1e-05 C 0.01\n",
      "IMDB_valid_f1 0.869 when we use penalty l1 tolerence 1e-05 C 0.1\n",
      "IMDB_valid_f1 0.8492 when we use penalty l1 tolerence 1e-05 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.0001 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 0.0001 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 0.0001 C 0.01\n",
      "IMDB_valid_f1 0.869 when we use penalty l1 tolerence 0.0001 C 0.1\n",
      "IMDB_valid_f1 0.8451 when we use penalty l1 tolerence 0.0001 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.001 C 0.0001\n",
      "IMDB_valid_f1 0.7042 when we use penalty l1 tolerence 0.001 C 0.001\n",
      "IMDB_valid_f1 0.8344 when we use penalty l1 tolerence 0.001 C 0.01\n",
      "IMDB_valid_f1 0.8691000000000001 when we use penalty l1 tolerence 0.001 C 0.1\n",
      "IMDB_valid_f1 0.8454 when we use penalty l1 tolerence 0.001 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.01 C 0.0001\n",
      "IMDB_valid_f1 0.7043 when we use penalty l1 tolerence 0.01 C 0.001\n",
      "IMDB_valid_f1 0.8347 when we use penalty l1 tolerence 0.01 C 0.01\n",
      "IMDB_valid_f1 0.8692 when we use penalty l1 tolerence 0.01 C 0.1\n",
      "IMDB_valid_f1 0.848 when we use penalty l1 tolerence 0.01 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 0.1 C 0.0001\n",
      "IMDB_valid_f1 0.7017 when we use penalty l1 tolerence 0.1 C 0.001\n",
      "IMDB_valid_f1 0.8355 when we use penalty l1 tolerence 0.1 C 0.01\n",
      "IMDB_valid_f1 0.8699000000000001 when we use penalty l1 tolerence 0.1 C 0.1\n",
      "IMDB_valid_f1 0.8565 when we use penalty l1 tolerence 0.1 C 1\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 1e-10\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 1e-09\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 1e-08\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 1e-07\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 1e-06\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 1e-05\n",
      "IMDB_valid_f1 0.5 when we use penalty l1 tolerence 1 C 0.0001\n",
      "IMDB_valid_f1 0.7066 when we use penalty l1 tolerence 1 C 0.001\n",
      "IMDB_valid_f1 0.8358 when we use penalty l1 tolerence 1 C 0.01\n",
      "IMDB_valid_f1 0.8697999999999999 when we use penalty l1 tolerence 1 C 0.1\n",
      "IMDB_valid_f1 0.8566 when we use penalty l1 tolerence 1 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-10 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-10 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-10 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-10 C 1e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-10 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-10 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-10 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-10 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-10 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-10 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-10 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-09 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-09 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-09 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-09 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-09 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-09 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-09 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-09 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-09 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-09 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-09 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-08 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-08 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-08 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-08 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-08 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-08 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-08 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-08 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-08 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-08 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-08 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-07 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-07 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-07 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-07 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-07 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-07 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-07 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-07 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-07 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-07 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-07 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-06 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-06 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-06 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-06 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-06 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-06 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-06 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-06 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-06 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-06 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-06 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-05 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-05 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-05 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-05 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-05 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-05 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-05 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-05 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-05 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-05 C 0.1\n",
      "IMDB_valid_f1 0.8428 when we use penalty l2 tolerence 1e-05 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.0001 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.0001 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 0.0001 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 0.0001 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 0.0001 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 0.0001 C 1e-05\n",
      "IMDB_valid_f1 0.8316 when we use penalty l2 tolerence 0.0001 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 0.0001 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 0.0001 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 0.0001 C 0.1\n",
      "IMDB_valid_f1 0.8428 when we use penalty l2 tolerence 0.0001 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.001 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.001 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 0.001 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 0.001 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 0.001 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 0.001 C 1e-05\n",
      "IMDB_valid_f1 0.8316 when we use penalty l2 tolerence 0.001 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 0.001 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 0.001 C 0.01\n",
      "IMDB_valid_f1 0.8572 when we use penalty l2 tolerence 0.001 C 0.1\n",
      "IMDB_valid_f1 0.8446 when we use penalty l2 tolerence 0.001 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.01 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.01 C 1e-09\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.01 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 0.01 C 1e-07\n",
      "IMDB_valid_f1 0.7101 when we use penalty l2 tolerence 0.01 C 1e-06\n",
      "IMDB_valid_f1 0.7905 when we use penalty l2 tolerence 0.01 C 1e-05\n",
      "IMDB_valid_f1 0.8317 when we use penalty l2 tolerence 0.01 C 0.0001\n",
      "IMDB_valid_f1 0.8677 when we use penalty l2 tolerence 0.01 C 0.001\n",
      "IMDB_valid_f1 0.8744 when we use penalty l2 tolerence 0.01 C 0.01\n",
      "IMDB_valid_f1 0.856 when we use penalty l2 tolerence 0.01 C 0.1\n",
      "IMDB_valid_f1 0.8476 when we use penalty l2 tolerence 0.01 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-09\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-08\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-07\n",
      "IMDB_valid_f1 0.7101 when we use penalty l2 tolerence 0.1 C 1e-06\n",
      "IMDB_valid_f1 0.7905 when we use penalty l2 tolerence 0.1 C 1e-05\n",
      "IMDB_valid_f1 0.8317 when we use penalty l2 tolerence 0.1 C 0.0001\n",
      "IMDB_valid_f1 0.8688 when we use penalty l2 tolerence 0.1 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 0.1 C 0.01\n",
      "IMDB_valid_f1 0.8557999999999999 when we use penalty l2 tolerence 0.1 C 0.1\n",
      "IMDB_valid_f1 0.8578 when we use penalty l2 tolerence 0.1 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-09\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-08\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-07\n",
      "IMDB_valid_f1 0.7101 when we use penalty l2 tolerence 1 C 1e-06\n",
      "IMDB_valid_f1 0.7791 when we use penalty l2 tolerence 1 C 1e-05\n",
      "IMDB_valid_f1 0.8238 when we use penalty l2 tolerence 1 C 0.0001\n",
      "IMDB_valid_f1 0.8623 when we use penalty l2 tolerence 1 C 0.001\n",
      "IMDB_valid_f1 0.8665000000000002 when we use penalty l2 tolerence 1 C 0.01\n",
      "IMDB_valid_f1 0.8609 when we use penalty l2 tolerence 1 C 0.1\n",
      "IMDB_valid_f1 0.8598 when we use penalty l2 tolerence 1 C 1\n"
     ]
    }
   ],
   "source": [
    "IMDB_valid_f1 = []\n",
    "IMDB_valid_acc = []\n",
    "for itr1 in range(len(pen)):\n",
    "    for itr2 in range(len(tolerance)):\n",
    "        for itr3 in range(len(C_param)):\n",
    "            linear_clf = LinearSVC(penalty=pen[itr1],tol=tolerance[itr2],C=C_param[itr3],dual=False).fit(train_mat, train_y_true)\n",
    "            f1 , acc = getClassifierEff (valid_mat,valid_y_true,linear_clf)\n",
    "            IMDB_valid_f1.append(f1)\n",
    "            IMDB_valid_acc.append(acc)\n",
    "            \n",
    "            print('IMDB_valid_f1',f1,'when we use penalty',pen[itr1],'tolerence',tolerance[itr2],'C',C_param[itr3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_train_f1 0.9630666666666666\n",
      "IMDB_test_f1 0.86896\n",
      "IMDB_valid_f1 0.8744\n"
     ]
    }
   ],
   "source": [
    "linear_clf = LinearSVC(penalty='l2',tol=0.01,C=0.01,dual=False).fit(train_mat, train_y_true)\n",
    "IMDB_train_f1,IMDB_train_acc = getClassifierEff (train_mat,train_y_true,linear_clf)\n",
    "IMDB_test_f1,IMDB_test_acc = getClassifierEff (test_mat,test_y_true,linear_clf)\n",
    "IMDB_valid_f1,IMDB_valid_acc = getClassifierEff (valid_mat,valid_y_true,linear_clf)\n",
    "print('IMDB_train_f1',IMDB_train_f1)\n",
    "print('IMDB_test_f1',IMDB_test_f1)\n",
    "print('IMDB_valid_f1',IMDB_valid_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for combination of penalty='l2' , loss='hinge',dual=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pen = ['l2']\n",
    "los = ['hinge'] # gives error for hinge\n",
    "dul = [True] # gives error for dual = true. \n",
    "tolerance = [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1]\n",
    "C_param = [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-10 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-10 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-10 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-10 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-10 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-10 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-10 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-10 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-10 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-10 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-10 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-09 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-09 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-09 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-09 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-09 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-09 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-09 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-09 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-09 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-09 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-09 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-08 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-08 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-08 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-08 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-08 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-08 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-08 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-08 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-08 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-08 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-08 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-07 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-07 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-07 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-07 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-07 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-07 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-07 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-07 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-07 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-07 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-07 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-06 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-06 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-06 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-06 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-06 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-06 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-06 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-06 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-06 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-06 C 0.1\n",
      "IMDB_valid_f1 0.8427 when we use penalty l2 tolerence 1e-06 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-05 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1e-05 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 1e-05 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 1e-05 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 1e-05 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 1e-05 C 1e-05\n",
      "IMDB_valid_f1 0.8315 when we use penalty l2 tolerence 1e-05 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 1e-05 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 1e-05 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 1e-05 C 0.1\n",
      "IMDB_valid_f1 0.8428 when we use penalty l2 tolerence 1e-05 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.0001 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.0001 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 0.0001 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 0.0001 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 0.0001 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 0.0001 C 1e-05\n",
      "IMDB_valid_f1 0.8316 when we use penalty l2 tolerence 0.0001 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 0.0001 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 0.0001 C 0.01\n",
      "IMDB_valid_f1 0.8571000000000001 when we use penalty l2 tolerence 0.0001 C 0.1\n",
      "IMDB_valid_f1 0.8428 when we use penalty l2 tolerence 0.0001 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.001 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.001 C 1e-09\n",
      "IMDB_valid_f1 0.6 when we use penalty l2 tolerence 0.001 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 0.001 C 1e-07\n",
      "IMDB_valid_f1 0.7100000000000001 when we use penalty l2 tolerence 0.001 C 1e-06\n",
      "IMDB_valid_f1 0.7907 when we use penalty l2 tolerence 0.001 C 1e-05\n",
      "IMDB_valid_f1 0.8316 when we use penalty l2 tolerence 0.001 C 0.0001\n",
      "IMDB_valid_f1 0.8673 when we use penalty l2 tolerence 0.001 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 0.001 C 0.01\n",
      "IMDB_valid_f1 0.8572 when we use penalty l2 tolerence 0.001 C 0.1\n",
      "IMDB_valid_f1 0.8446 when we use penalty l2 tolerence 0.001 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.01 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.01 C 1e-09\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.01 C 1e-08\n",
      "IMDB_valid_f1 0.614 when we use penalty l2 tolerence 0.01 C 1e-07\n",
      "IMDB_valid_f1 0.7101 when we use penalty l2 tolerence 0.01 C 1e-06\n",
      "IMDB_valid_f1 0.7905 when we use penalty l2 tolerence 0.01 C 1e-05\n",
      "IMDB_valid_f1 0.8317 when we use penalty l2 tolerence 0.01 C 0.0001\n",
      "IMDB_valid_f1 0.8677 when we use penalty l2 tolerence 0.01 C 0.001\n",
      "IMDB_valid_f1 0.8744 when we use penalty l2 tolerence 0.01 C 0.01\n",
      "IMDB_valid_f1 0.856 when we use penalty l2 tolerence 0.01 C 0.1\n",
      "IMDB_valid_f1 0.8476 when we use penalty l2 tolerence 0.01 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-09\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-08\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 0.1 C 1e-07\n",
      "IMDB_valid_f1 0.7101 when we use penalty l2 tolerence 0.1 C 1e-06\n",
      "IMDB_valid_f1 0.7905 when we use penalty l2 tolerence 0.1 C 1e-05\n",
      "IMDB_valid_f1 0.8317 when we use penalty l2 tolerence 0.1 C 0.0001\n",
      "IMDB_valid_f1 0.8688 when we use penalty l2 tolerence 0.1 C 0.001\n",
      "IMDB_valid_f1 0.8741 when we use penalty l2 tolerence 0.1 C 0.01\n",
      "IMDB_valid_f1 0.8557999999999999 when we use penalty l2 tolerence 0.1 C 0.1\n",
      "IMDB_valid_f1 0.8578 when we use penalty l2 tolerence 0.1 C 1\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-10\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-09\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-08\n",
      "IMDB_valid_f1 0.5979 when we use penalty l2 tolerence 1 C 1e-07\n",
      "IMDB_valid_f1 0.7101 when we use penalty l2 tolerence 1 C 1e-06\n",
      "IMDB_valid_f1 0.7791 when we use penalty l2 tolerence 1 C 1e-05\n",
      "IMDB_valid_f1 0.8238 when we use penalty l2 tolerence 1 C 0.0001\n",
      "IMDB_valid_f1 0.8623 when we use penalty l2 tolerence 1 C 0.001\n",
      "IMDB_valid_f1 0.8665000000000002 when we use penalty l2 tolerence 1 C 0.01\n",
      "IMDB_valid_f1 0.8609 when we use penalty l2 tolerence 1 C 0.1\n",
      "IMDB_valid_f1 0.8598 when we use penalty l2 tolerence 1 C 1\n"
     ]
    }
   ],
   "source": [
    "IMDB_valid_f1 = []\n",
    "IMDB_valid_acc = []\n",
    "for itr1 in range(len(pen)):\n",
    "    for itr2 in range(len(tolerance)):\n",
    "        for itr3 in range(len(C_param)):\n",
    "            linear_clf = LinearSVC(penalty=pen[itr1],tol=tolerance[itr2],C=C_param[itr3],dual=False).fit(train_mat, train_y_true)\n",
    "            f1 , acc = getClassifierEff (valid_mat,valid_y_true,linear_clf)\n",
    "            IMDB_valid_f1.append(f1)\n",
    "            IMDB_valid_acc.append(acc)\n",
    "            \n",
    "            print('IMDB_valid_f1',f1,'when we use penalty',pen[itr1],'tolerence',tolerance[itr2],'C',C_param[itr3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB_train_f1 0.9286666666666666\n",
      "IMDB_test_f1 0.87072\n",
      "IMDB_valid_f1 0.8736000000000002\n"
     ]
    }
   ],
   "source": [
    "linear_clf = LinearSVC(penalty='l2',tol=0.01,C=0.01,loss='hinge',dual=True).fit(train_mat, train_y_true)\n",
    "IMDB_train_f1,IMDB_train_acc = getClassifierEff (train_mat,train_y_true,linear_clf)\n",
    "IMDB_test_f1,IMDB_test_acc = getClassifierEff (test_mat,test_y_true,linear_clf)\n",
    "IMDB_valid_f1,IMDB_valid_acc = getClassifierEff (valid_mat,valid_y_true,linear_clf)\n",
    "print('IMDB_train_f1',IMDB_train_f1)\n",
    "print('IMDB_test_f1',IMDB_test_f1)\n",
    "print('IMDB_valid_f1',IMDB_valid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
