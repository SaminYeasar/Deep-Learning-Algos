{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machine learning\\python\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('MNIST_data')\n",
    "# separate train and test data\n",
    "train_data = data.train.images\n",
    "train_label = data.train.labels\n",
    "\n",
    "test_data = data.test.images\n",
    "test_label = data.test.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "img_len  = train_data.shape[1]\n",
    "img_size = np.int(np.sqrt(img_len))\n",
    "print(img_len)\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape (55000, 784)\n",
      "train label length (55000,)\n",
      "test data shape (10000, 784)\n",
      "test label length (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('train_data shape',train_data.shape)\n",
    "print('train label length',train_label.shape)\n",
    "print('test data shape',test_data.shape)\n",
    "print('test label length',test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    #p = np.zeros((1,m))\n",
    "    predicted_class = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    predicted_class = np.argmax(probas,axis=0).reshape(1,-1)\n",
    "    true_class = np.argmax(y,axis=0).reshape(1,-1)\n",
    "    acc = np.mean(predicted_class == true_class)\n",
    "    #print ( 'training accuracy: %.2f' % (np.mean(predicted_class == true_class)) )\n",
    "    '''\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "    '''    \n",
    "    return predicted_class,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convt_oneHotcoded(labels,no_class):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    import numpy as np\n",
    "    data_len = labels.shape[1]\n",
    "    indptr = np.arange(0,data_len+1)\n",
    "    indices = labels.reshape(-1)\n",
    "    data = np.ones(data_len)\n",
    "\n",
    "    output_mat =  csr_matrix((data, indices, indptr), shape=(data_len,no_class)).toarray() \n",
    "    output_mat = output_mat.T\n",
    "    # shape=(data_len, 10),  here we know out put labels ranges from 0 to 10\n",
    "    return output_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_x(data, labels, num):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    no_example = data.shape[1]\n",
    "    idx = np.arange(0 , no_example)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[:,idx]\n",
    "    labels_shuffle = labels[:,idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainTest (data,label):\n",
    "  #  train_data = []\n",
    "  #  train_label = []\n",
    "  #  test_data = []\n",
    "  #  test_label = []\n",
    "    \n",
    "    import numpy as np\n",
    "    np.random.seed(3)\n",
    "    idx = np.arange(data.shape[1])\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_data = data[:,0:np.int(len(idx)*0.8)]\n",
    "    train_label = label[:,0:np.int(len(idx)*0.8)]\n",
    "\n",
    "    test_data =  data[:,np.int(len(idx)*0.8):]\n",
    "    test_label =  label[:,np.int(len(idx)*0.8):]\n",
    "\n",
    "    #train_label = np.squeeze (train_label)\n",
    "    return train_data,train_label,test_data,test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation module\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{1}$$\n",
    "$$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\\tag{2}$$ \n",
    "where the activation \"g\" can be sigmoid() or relu(). Use linear_forward() and the correct activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.\n",
    "\n",
    "**Exercise**: Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    '''\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -(1/m) * ( np.dot(Y,np.log(AL).T) + np.dot(1-Y,np.log(1-AL).T) )\n",
    "    ### END CODE HERE ###\n",
    "    '''\n",
    "    cost = -(1/m) * np.sum (np.multiply(Y,np.log(AL)) + np.multiply(1-Y,np.log(1-AL)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(AL.shape)\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation module\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{7}$$. \n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-layer Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(train_x, train_y,valid_x, valid_y, layers_dims, learning_rate , num_iterations , print_cost):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "        # learning rate decay\n",
    "    max_learning_rate = 0.1\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "        \n",
    "        X, Y = next_batch_x(train_x, train_y, 100)\n",
    "        \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            pred_train ,c1 = predict(train_x, train_y, parameters)\n",
    "            train_acc.append(c1)\n",
    "            pred_valid , c2 = predict(valid_x, valid_y, parameters)\n",
    "            valid_acc.append(c2)\n",
    "            \n",
    "    # plot the cost\n",
    "    '''\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    '''\n",
    "    return parameters,costs,train_acc,valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data shape = (no features , no example )\n",
    "                \n",
    "                X11...................... \n",
    "                .........................\n",
    "                \n",
    "                X1 X2 ............Xm-1 Xm\n",
    "                \n",
    "                .........................\n",
    "                X1n......................\n",
    "                \n",
    "                Here we have n number \"features\" and m \"examples\"\n",
    "                \n",
    "target shape = ( no class, output )\n",
    "\n",
    "                1 0 0 1 ............0 0 0\n",
    "                0 0 0 0 ............1 0 0\n",
    "                0 1 1 0.............0 1 1\n",
    "                \n",
    "                here we have 3 possible \"class\" and m number of \"output\" prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to one hot encoded\n",
    "train_label = train_label.reshape(1,-1)\n",
    "test_label = test_label.reshape(1,-1)\n",
    "no_class=10\n",
    "train_label_coded = convt_oneHotcoded(train_label,no_class)  \n",
    "test_label_coded = convt_oneHotcoded(test_label,no_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input dat to desired form\n",
    "train_x = train_data.T\n",
    "train_y = train_label_coded\n",
    "\n",
    "test_x = test_data.T\n",
    "test_y = test_label_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape (784, 55000)\n",
      "train label length (10, 55000)\n",
      "validation data shape (784, 10000)\n",
      "validation label length (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print('train_data shape',train_x.shape)\n",
    "print('train label length',train_y.shape)\n",
    "print('validation data shape',test_x.shape)\n",
    "print('validation label length',test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 6.931501\n",
      "Cost after iteration 100: 3.250597\n",
      "Cost after iteration 200: 3.243149\n",
      "Cost after iteration 300: 3.251132\n",
      "Cost after iteration 400: 3.223263\n",
      "Cost after iteration 500: 3.092954\n",
      "Cost after iteration 600: 2.775511\n",
      "Cost after iteration 700: 2.730778\n",
      "Cost after iteration 800: 2.526592\n",
      "Cost after iteration 900: 2.274263\n",
      "Cost after iteration 1000: 1.637507\n",
      "Cost after iteration 1100: 1.374610\n",
      "Cost after iteration 1200: 1.227500\n",
      "Cost after iteration 1300: 1.116401\n",
      "Cost after iteration 1400: 0.891751\n",
      "Cost after iteration 1500: 0.774110\n",
      "Cost after iteration 1600: 0.591070\n",
      "Cost after iteration 1700: 0.683616\n",
      "Cost after iteration 1800: 0.516621\n",
      "Cost after iteration 1900: 0.566959\n",
      "Cost after iteration 2000: 0.713916\n",
      "Cost after iteration 2100: 0.574611\n",
      "Cost after iteration 2200: 0.504406\n",
      "Cost after iteration 2300: 0.378346\n",
      "Cost after iteration 2400: 0.617077\n",
      "Cost after iteration 2500: 0.339932\n",
      "Cost after iteration 2600: 0.455139\n",
      "Cost after iteration 2700: 0.288542\n",
      "Cost after iteration 2800: 0.330967\n",
      "Cost after iteration 2900: 0.303680\n",
      "Cost after iteration 3000: 0.313848\n",
      "Cost after iteration 3100: 0.464748\n",
      "Cost after iteration 3200: 0.397571\n",
      "Cost after iteration 3300: 0.459465\n",
      "Cost after iteration 3400: 0.250194\n",
      "Cost after iteration 3500: 0.335949\n",
      "Cost after iteration 3600: 0.248633\n",
      "Cost after iteration 3700: 0.333564\n",
      "Cost after iteration 3800: 0.336266\n",
      "Cost after iteration 3900: 0.212979\n",
      "Cost after iteration 4000: 0.472662\n",
      "Cost after iteration 4100: 0.483241\n",
      "Cost after iteration 4200: 0.239527\n",
      "Cost after iteration 4300: 0.288879\n",
      "Cost after iteration 4400: 0.269725\n",
      "Cost after iteration 4500: 0.125991\n",
      "Cost after iteration 4600: 0.319637\n",
      "Cost after iteration 4700: 0.261012\n",
      "Cost after iteration 4800: 0.279146\n",
      "Cost after iteration 4900: 0.238365\n",
      "Cost after iteration 5000: 0.226421\n",
      "Cost after iteration 5100: 0.165845\n",
      "Cost after iteration 5200: 0.267672\n",
      "Cost after iteration 5300: 0.217720\n",
      "Cost after iteration 5400: 0.154774\n",
      "Cost after iteration 5500: 0.343847\n",
      "Cost after iteration 5600: 0.494587\n",
      "Cost after iteration 5700: 0.329172\n",
      "Cost after iteration 5800: 0.192742\n",
      "Cost after iteration 5900: 0.126101\n",
      "Cost after iteration 6000: 0.337651\n",
      "Cost after iteration 6100: 0.186416\n",
      "Cost after iteration 6200: 0.306061\n",
      "Cost after iteration 6300: 0.312780\n",
      "Cost after iteration 6400: 0.180271\n",
      "Cost after iteration 6500: 0.190447\n",
      "Cost after iteration 6600: 0.199147\n",
      "Cost after iteration 6700: 0.181492\n",
      "Cost after iteration 6800: 0.287431\n",
      "Cost after iteration 6900: 0.275874\n",
      "Cost after iteration 7000: 0.194908\n",
      "Cost after iteration 7100: 0.170961\n",
      "Cost after iteration 7200: 0.142325\n",
      "Cost after iteration 7300: 0.157064\n",
      "Cost after iteration 7400: 0.164668\n",
      "Cost after iteration 7500: 0.187363\n",
      "Cost after iteration 7600: 0.295783\n",
      "Cost after iteration 7700: 0.103416\n",
      "Cost after iteration 7800: 0.270680\n",
      "Cost after iteration 7900: 0.210363\n",
      "Cost after iteration 8000: 0.115933\n",
      "Cost after iteration 8100: 0.209722\n",
      "Cost after iteration 8200: 0.326728\n",
      "Cost after iteration 8300: 0.280051\n",
      "Cost after iteration 8400: 0.140401\n",
      "Cost after iteration 8500: 0.173677\n",
      "Cost after iteration 8600: 0.190952\n",
      "Cost after iteration 8700: 0.181871\n",
      "Cost after iteration 8800: 0.225889\n",
      "Cost after iteration 8900: 0.334380\n",
      "Cost after iteration 9000: 0.327023\n",
      "Cost after iteration 9100: 0.209967\n",
      "Cost after iteration 9200: 0.363930\n",
      "Cost after iteration 9300: 0.252465\n",
      "Cost after iteration 9400: 0.300457\n",
      "Cost after iteration 9500: 0.250664\n",
      "Cost after iteration 9600: 0.551110\n",
      "Cost after iteration 9700: 0.434844\n",
      "Cost after iteration 9800: 0.163213\n",
      "Cost after iteration 9900: 0.150220\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [784, 400, 400, 200, 10]\n",
    "parameters,costs,pred_train,pred_valid = L_layer_model(train_x, train_y,test_x, test_y, layers_dims,learning_rate = 0.0001, num_iterations = 10000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155000bc080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(100,10001,100),pred_train,label='accuracy of training data')\n",
    "plt.plot(np.arange(100,10001,100),pred_valid,label='accuracy of test data')\n",
    "plt.xlabel('no of iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('train_validation_H3.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1553bbf74e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(100,10001,100),costs,label='trainning loss')\n",
    "plt.xlabel('no of iterations')\n",
    "plt.legend()\n",
    "plt.savefig('train_loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 96.72545454545455\n",
      "Test accuracy: 96.1\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy:',pred_train[-1]*100)\n",
    "print('Test accuracy:',pred_valid[-1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
